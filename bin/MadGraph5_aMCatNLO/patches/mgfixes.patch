diff -ur -x '.*/models/.*' ../old/MG5_aMC_v2_3_3/madgraph/interface/reweight_interface.py MG5_aMC_v2_3_3/madgraph/interface/reweight_interface.py
--- ../old/MG5_aMC_v2_3_3/madgraph/interface/reweight_interface.py	2015-10-26 02:18:52.000000001 +0100
+++ MG5_aMC_v2_3_3/madgraph/interface/reweight_interface.py	2015-11-18 20:27:57.000000001 +0100
@@ -782,7 +782,8 @@
                     os.remove(pjoin(Pdir, 'matrix%spy.so' % newtag))
                     newtag  = "L%s" % newtag
                     os.environ['MENUM'] = newtag
-                    misc.compile(['matrix%spy.so' % newtag], cwd=Pdir)
+                    if not self.rwgt_dir or not os.path.exists(pjoin(Pdir,'matrix%spy.so' % newtag )):
+                        misc.compile(['matrix%spy.so' % newtag], cwd=Pdir)
                     mymod = __import__('rw_me.SubProcesses.%s.matrix%spy' % (Pname, newtag), globals(), locals(), [],-1)
                 
                 S = mymod.SubProcesses
@@ -818,7 +819,8 @@
                 os.remove(pjoin(Pdir, 'matrix%spy.so' % metag ))
                 metag = "L%s" % metag
                 os.environ['MENUM'] = str(metag)
-                misc.compile(['matrix%spy.so' % metag], cwd=pjoin(subdir, Pdir))
+                if not self.rwgt_dir or not os.path.exists(pjoin(Pdir,'matrix%spy.so' % metag)):
+                    misc.compile(['matrix%spy.so' % metag], cwd=pjoin(subdir, Pdir))
                 mymod = __import__("rw_me_second.SubProcesses.%s.matrix%spy" % (Pname, metag))
             reload(mymod)
             S = mymod.SubProcesses
diff -ur -x '.*/models/.*' ../old/MG5_aMC_v2_3_3/madgraph/various/cluster.py MG5_aMC_v2_3_3/madgraph/various/cluster.py
--- ../old/MG5_aMC_v2_3_3/madgraph/various/cluster.py	2015-10-26 02:18:52.000000001 +0100
+++ MG5_aMC_v2_3_3/madgraph/various/cluster.py	2015-12-11 15:13:21.000000001 +0100
@@ -19,6 +19,11 @@
 import glob
 import inspect
 import sys
+import platform
+import signal
+import uuid
+import socket
+import atexit
 
 logger = logging.getLogger('madgraph.cluster') 
 
@@ -83,6 +88,10 @@
     else:
         return True
 
+def cleansubproc(subproc):
+    #print "killing %s" % subproc
+    subproc.terminate()
+
 class Cluster(object):
     """Basic Class for all cluster type submission"""
     name = 'mother class'
@@ -97,6 +106,7 @@
         self.submitted_dirs = [] #HTCaaS
         self.submitted_exes = [] #HTCaaS
         self.submitted_args = [] #HTCaaS
+        self.hold_msg = ""
 
         if 'cluster_queue' in opts:
             self.cluster_queue = opts['cluster_queue']
@@ -307,7 +317,8 @@
             else:
                 nb_job = idle + run + finish + fail
             if fail:
-                raise ClusterManagmentError('Some Jobs are in a Hold/... state. Please try to investigate or contact the IT team')
+                raise ClusterManagmentError('Some Jobs are in a Hold/... state. Error messages are below.' 
+                        'Please try to investigate or contact the IT team. \n%s' % self.hold_msg)
             if idle + run == 0:
                 #time.sleep(20) #security to ensure that the file are really written on the disk
                 logger.info('All jobs finished')
@@ -840,6 +851,7 @@
                   Universe = vanilla
                   notification = Error
                   Initialdir = %(cwd)s
+                  Request_memory = 528
                   %(requirement)s
                   getenv=True
                   queue 1
@@ -916,6 +928,7 @@
                   Universe = vanilla
                   notification = Error
                   Initialdir = %(cwd)s
+                  Request_memory = 528
                   %(requirement)s
                   getenv=True
                   queue 1
@@ -1023,6 +1036,13 @@
                     idle += 1
                 elif status == 'R':
                     run += 1
+                elif status == 'H':
+                    error = misc.Popen(["condor_q", "-format", "'%s\n'", "HoldReason", id], 
+                                            stdout=subprocess.PIPE)
+                    self.hold_msg += "Hold message for ID %s:" % id
+                    for line in error.stdout:
+                        self.hold_msg += line
+                    fail += 1
                 elif status != 'C':
                     fail += 1
 
@@ -1349,6 +1358,80 @@
     name = 'lsf'
     job_id = 'LSB_JOBID'
 
+    def __init__(self,*args, **opts):
+        """Init the cluster"""
+        Cluster.__init__(self,*args, **opts)
+
+        if self.temp_dir!=None:
+            self.dorsync = True
+            #print "starting rsync"
+          
+            cwd = os.getcwd()
+
+            self.rsyncroot = cwd
+            
+            self.rsyncmodule = str(uuid.uuid4())
+            
+            #get free port number for rsyncd
+            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            sock.bind(('localhost', 0))
+            addr, port = sock.getsockname()
+            sock.close()    
+            
+            self.rsyncport = port
+            #print self.rsyncport
+            
+            rsynclog = os.path.join(cwd, 'rsyncd_%i.log' % self.rsyncport)
+            rsynclock = os.path.join(cwd, 'rsyncd_%i.lock' % self.rsyncport)
+            rsyncpid = os.path.join(cwd, 'rsyncd_%i.pid' % self.rsyncport)
+
+            rsyncpasswd = str(uuid.uuid4())
+            
+            self.rsyncuser = 'madgraph'
+            
+            rsyncsecrets = "%s:%s" % (self.rsyncuser,rsyncpasswd)
+            rsyncsecretsfile = os.path.join(cwd, 'rsyncsecrets_%i' % self.rsyncport)
+            secretsh = open(rsyncsecretsfile,'w')
+            os.chmod(rsyncsecretsfile, 0600)
+            secretsh.write(rsyncsecrets)
+          
+            os.environ["MADGRAPHRSYNCPASSWD_%i" % self.rsyncport] = rsyncpasswd
+            #print rsyncpasswd
+
+            rsyncconf = """
+              port = %(rsyncport)s
+              pid file = %(rsyncpid)s
+              log file = %(rsynclog)s
+             
+                [%(rsyncmodule)s]
+                  comment = Random things available for download
+                  lock file = %(rsynclock)s
+                  secrets file = %(rsyncsecrets)s
+                  path = %(path)s
+                  list = yes 
+                  use chroot = no
+                  munge symlinks = no
+                  read only = no
+                  auth users = %(rsyncuser)s
+            """ % {'rsyncport': self.rsyncport,
+                   'rsyncmodule': self.rsyncmodule,
+                   'path': cwd,
+                  'rsynclog' : rsynclog,
+                  'rsynclock' : rsynclock,
+                  'rsyncpid' : rsyncpid,
+                  'rsyncsecrets' : rsyncsecretsfile,
+                  'rsyncuser' : self.rsyncuser,
+                  }
+            
+            rsyncconffile = os.path.join(cwd, 'rsyncd_%i.conf' % self.rsyncport)
+            open(rsyncconffile,'w').write(rsyncconf)
+            
+            self.rsyncd = subprocess.Popen(['rsync','--daemon', '--no-detach', '--config=%s' % rsyncconffile],cwd=cwd,stdout=subprocess.PIPE,stdin=subprocess.PIPE,stderr=subprocess.PIPE)
+            atexit.register(cleansubproc,self.rsyncd)
+            
+        else:
+            self.dorsync = False
+
     @multiple_try()
     def submit(self, prog, argument=[], cwd=None, stdout=None, stderr=None, log=None,
                required_output=[], nb_submit=0):
@@ -1372,6 +1455,8 @@
         if log is None:
             log = '/dev/null'
         
+        text += 'if [ -n $CMSSW_BASE ]; then cd $CMSSW_BASE; eval `scramv1 runtime -sh`; cd -; fi;'
+        
         text += prog
         if argument:
             text += ' ' + ' '.join(argument)
@@ -1397,6 +1482,136 @@
         self.submitted_ids.append(id)
         return id        
         
+    @store_input()
+    @multiple_try()
+    def submit2(self, prog, argument=[], cwd=None, stdout=None, stderr=None,
+            log=None, input_files=[], output_files=[], required_output=[],nb_submit=0):
+        """How to make one submission. Return status id on the cluster.
+        NO SHARE DISK"""
+
+        #print "running lsf submit2"
+
+        if cwd is None:
+            cwd = os.getcwd()
+        if not os.path.exists(prog):
+            prog = os.path.join(cwd, prog)
+
+        if not required_output and output_files:
+            required_output = output_files
+
+        if not self.dorsync or (not input_files and not output_files):
+            # not input/output so not using submit2
+            return self.submit(prog, argument, cwd, stdout, stderr, log,
+        required_output=required_output, nb_submit=nb_submit)
+
+        if self.rsyncd.poll()!=None:
+            raise RuntimeError("rsyncd not running")
+
+        if cwd is None:
+            cwd = os.getcwd()
+        if not os.path.exists(prog):
+            prog = os.path.join(cwd, prog)
+        temp_file_name = "sub." + os.path.basename(prog) + '.'.join(argument)
+               
+        input_files.append(prog)                
+               
+        hostname = platform.node()
+               
+        rsynccwd = cwd
+        if rsynccwd.startswith(self.rsyncroot):
+            rsynccwd = rsynccwd[len(self.rsyncroot):]                   
+               
+        infilelist = ""
+        for input_file in input_files:
+            #make sure input_files are absolute paths
+            if not input_file.startswith('/'):
+                input_file = os.path.join(cwd,input_file)
+            #convert to paths relative to rsyncd root
+            if input_file.startswith(self.rsyncroot):
+                input_file = input_file[len(self.rsyncroot):]
+            infilelist += "%s@%s::%s/%s " % (self.rsyncuser,hostname,self.rsyncmodule, input_file)
+        infilelist += "./"
+        
+        outfilelist = ""
+        for output_file in output_files:
+            outfilelist += "%s " % (output_file)
+        outfilelist += "%s@%s::%s/%s" % (self.rsyncuser,hostname,self.rsyncmodule,rsynccwd)
+            
+        text = """#!/bin/bash
+        
+            SUBMITTERHOST=%(hostname)s            
+
+            if [ -n $CMSSW_VERSION ]
+            then
+              scramv1 project CMSSW $CMSSW_VERSION
+              cd $CMSSW_VERSION
+              eval `scramv1 runtime -sh`
+              cd -
+            fi
+                             
+            export RSYNC_PASSWORD=$MADGRAPHRSYNCPASSWD_%(rsyncport)s
+                 
+            #dereference symlinks for input
+            rsync -vvv --timeout=600 --contimeout=600 --port %(rsyncport)s -rptL %(infilelist)s
+
+            echo '%(arguments)s' > arguments
+            chmod +x ./%(script)s        
+            %(program)s ./%(script)s %(arguments)s
+            
+            #copy symlinks as symlinks for output and don't overwrite existing files unless updated
+            rsync -vvv --timeout=600 --contimeout=600 --port %(rsyncport)s -rptul %(outfilelist)s
+            
+            """
+        dico = {'script': os.path.basename(prog),
+        'hostname': hostname,
+        'infilelist': infilelist,
+        'outfilelist': outfilelist,
+        'output_files': ' '.join(output_files),
+        'rsyncport': self.rsyncport,
+        'arguments': ' '.join([str(a) for a in argument]),
+        'program': ' ' if '.py' in prog else 'bash'}
+
+        me_dir = self.get_jobs_identifier(cwd, prog)
+
+        text = text % dico
+        cwdpath = "/tmp/" + os.environ.get("USER", '')
+        command = ['bsub', '-cwd', cwdpath, '-C0', '-J', me_dir]
+        if cwd is None:
+            cwd = os.getcwd()
+        #else:
+            #text += " cd %s;" % cwd
+        if stdout and isinstance(stdout, str):
+            command.extend(['-o', stdout])
+        if stderr and isinstance(stdout, str):
+            command.extend(['-e', stderr])
+        elif stderr == -2: # -2 is subprocess.STDOUT
+            pass
+        if log is None:
+            log = '/dev/null'
+
+        if self.cluster_queue and self.cluster_queue != 'None':
+            command.extend(['-q', self.cluster_queue])
+
+        submitenv = os.environ.copy()
+        submitenv["TMPDIR"] = "/tmp/" + submitenv.get("USER", '')
+        a = misc.Popen(command, stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        stdin=subprocess.PIPE, cwd=cwd,
+        env=submitenv)
+
+        output = a.communicate(text)[0]
+        #Job <nnnn> is submitted to default queue <normal>.
+        try:
+            id = output.split('>',1)[0].split('<')[1]
+        except:
+            raise ClusterManagmentError, 'fail to submit to the cluster: \n%s' \
+            % output
+        if not id.isdigit():
+            raise ClusterManagmentError, 'fail to submit to the cluster: \n%s' \
+            % output
+        self.submitted += 1
+        self.submitted_ids.append(id)
+        return id         
         
     @multiple_try()
     def control_one_job(self, id):
@@ -1428,37 +1643,48 @@
         
         if not self.submitted_ids:
             return 0, 0, 0, 0
-        
-        cmd = "bjobs " + ' '.join(self.submitted_ids) 
-        status = misc.Popen([cmd], shell=True, stdout=subprocess.PIPE)
 
         jobstatus = {}
-        for line in status.stdout:
-            line = line.strip()
-            if 'JOBID' in line:
-                continue
-            splitline = line.split()
-            id = splitline[0]
-            if id not in self.submitted_ids:
-                continue
-            jobstatus[id] = splitline[2]
+        
+        #split into smaller groups of 200 jobs to avoid problems with truncated output
+        idsplitting = 200
+        splitids = [self.submitted_ids[i:i+idsplitting] for i in range(0, len(self.submitted_ids), idsplitting)]
+        
+        for ids in splitids:
+            cmd = "bjobs " + ' '.join(ids) 
+            status = misc.Popen([cmd], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
+
+            for line in status.stdout:
+                line = line.strip()
+                if 'JOBID' in line:
+                    continue
+                splitline = line.split()
+                if splitline[0] in ids:
+                    id = splitline[0]
+                    jobstatus[id] = splitline[2]
+                else:
+                    splitline = re.split('[><]',line)
+                    if len(splitline)==3 and splitline[0] == 'Job ' and splitline[2] == " is not found" and splitline[1] in ids:
+                        id = splitline[1]
+                        jobstatus[id] = 'MISSING'
 
         idle, run, fail = 0, 0, 0
         for id in self.submitted_ids[:]:
             if id in jobstatus:
                 status = jobstatus[id]
             else:
-                status = 'MISSING'
+                status = 'PEND'
+                
             if status == 'RUN':
                 run += 1
-            elif status == 'PEND':
-                idle += 1
-            else:
+            elif status in ['DONE', 'EXIT', 'MISSING']:
                 status = self.check_termination(id)
                 if status == 'wait':
                     run += 1
                 elif status == 'resubmit':
-                    idle += 1                
+                    idle += 1
+            else:
+                idle += 1
 
         return idle, run, self.submitted - (idle+run+fail), fail
 
diff -ur -x '.*/models/.*' ../old/MG5_aMC_v2_3_3/MadSpin/interface_madspin.py MG5_aMC_v2_3_3/MadSpin/interface_madspin.py
--- ../old/MG5_aMC_v2_3_3/MadSpin/interface_madspin.py	2015-10-26 02:18:52.000000001 +0100
+++ MG5_aMC_v2_3_3/MadSpin/interface_madspin.py	2015-11-19 22:44:54.000000001 +0100
@@ -693,24 +693,56 @@
                     else:
                         mg5.exec_cmd("generate %s" % proc)
                         mg5.exec_cmd("output %s -f" % decay_dir)
-                options = dict(mg5.options)
-                import madgraph.interface.madevent_interface as madevent_interface
-                me5_cmd = madevent_interface.MadEventCmdShell(me_dir=os.path.realpath(\
+                    
+                    import madgraph.interface.madevent_interface as madevent_interface
+                    options = dict(mg5.options)
+                    if self.options['ms_dir']:
+                        misc.sprint("start gridpack!")
+                        # we are in gridpack mode -> create it
+                        me5_cmd = madevent_interface.MadEventCmdShell(me_dir=os.path.realpath(\
                                                 decay_dir), options=options)
-                me5_cmd.options["automatic_html_opening"] = False
-                if self.options["run_card"]:
-                    run_card = self.options["run_card"]
+                        me5_cmd.options["automatic_html_opening"] = False
+                        if self.options["run_card"]:
+                            run_card = self.options["run_card"]
+                        else:
+                            run_card = banner.RunCard(pjoin(decay_dir, "Cards", "run_card.dat"))                        
+                        
+                        run_card["iseed"] = self.seed
+                        run_card['gridpack'] = True
+                        run_card.write(pjoin(decay_dir, "Cards", "run_card.dat"))
+                        param_card = self.banner['slha']
+                        open(pjoin(decay_dir, "Cards", "param_card.dat"),"w").write(param_card)
+                        self.seed += 1
+                        # actually creation
+                        me5_cmd.exec_cmd("generate_events run_01 -f")
+                        me5_cmd.exec_cmd("exit")                        
+                        #remove pointless informat
+                        misc.call(["rm", "Cards", "bin", 'Source', 'SubProcesses'], cwd=decay_dir)
+                        misc.call(['tar', '-xzpvf', 'run_01_gridpack.tar.gz'], cwd=decay_dir)
+                
+                # Now generate the events
+
+                
+                if not self.options['ms_dir']:
+                    me5_cmd = madevent_interface.MadEventCmdShell(me_dir=os.path.realpath(\
+                                                    decay_dir), options=options)
+                    me5_cmd.options["automatic_html_opening"] = False
+                    if self.options["run_card"]:
+                        run_card = self.options["run_card"]
+                    else:
+                        run_card = banner.RunCard(pjoin(decay_dir, "Cards", "run_card.dat"))
+                    run_card["nevents"] = int(1.2*nb_event)
+                    run_card["iseed"] = self.seed
+                    run_card.write(pjoin(decay_dir, "Cards", "run_card.dat"))
+                    param_card = self.banner['slha']
+                    open(pjoin(decay_dir, "Cards", "param_card.dat"),"w").write(param_card)
+                    self.seed += 1
+                    me5_cmd.exec_cmd("generate_events run_01 -f")
+                    me5_cmd.exec_cmd("exit")
+                    out[i] = lhe_parser.EventFile(pjoin(decay_dir, "Events", 'run_01', 'unweighted_events.lhe.gz'))            
                 else:
-                    run_card = banner.RunCard(pjoin(decay_dir, "Cards", "run_card.dat"))
-                run_card["nevents"] = int(1.2*nb_event)
-                run_card["iseed"] = self.seed
-                run_card.write(pjoin(decay_dir, "Cards", "run_card.dat"))
-                param_card = self.banner['slha']
-                open(pjoin(decay_dir, "Cards", "param_card.dat"),"w").write(param_card)
-                self.seed += 1
-                me5_cmd.exec_cmd("generate_events run_01 -f")
-                me5_cmd.exec_cmd("exit")     
-                out[i] = lhe_parser.EventFile(pjoin(decay_dir, "Events", 'run_01', 'unweighted_events.lhe.gz'))            
+                    misc.call(['run.sh', str(int(1.2*nb_event)), str(self.seed)], cwd=decay_dir)     
+                    out[i] = lhe_parser.EventFile(pjoin(decay_dir, 'events.lhe.gz'))            
                 if cumul:
                     break
             
@@ -742,9 +774,10 @@
                     # final state and tag as to decay
                     to_decay[particle.pdg] += 1
 
+        import random
+
         # Handle the banner of the output file
         if not self.seed:
-            import random
             self.seed = random.randint(0, int(30081*30081))
             self.do_set('seed %s' % self.seed)
             logger.info('Will use seed %s' % self.seed)
Only in MG5_aMC_v2_3_3/models/loop_qcd_qed_sm: restrict_lepton_masses_no_lepton_yukawas.dat
Only in MG5_aMC_v2_3_3/models/loop_sm: restrict_ckm_no_b_mass.dat
Only in MG5_aMC_v2_3_3/models/sm: restrict_ckm_lepton_masses.dat
Only in MG5_aMC_v2_3_3/models/sm: restrict_ckm_lepton_masses_no_b_mass.dat
Only in MG5_aMC_v2_3_3/models/sm: restrict_ckm_no_b_mass.dat
Only in MG5_aMC_v2_3_3/models/sm: restrict_lepton_masses_no_b_mass.dat
diff -ur -x '.*/models/.*' ../old/MG5_aMC_v2_3_3/Template/LO/bin/internal/Gridpack/run.sh MG5_aMC_v2_3_3/Template/LO/bin/internal/Gridpack/run.sh
--- ../old/MG5_aMC_v2_3_3/Template/LO/bin/internal/Gridpack/run.sh	2015-10-26 02:18:52.000000001 +0100
+++ MG5_aMC_v2_3_3/Template/LO/bin/internal/Gridpack/run.sh	2015-11-18 17:21:02.000000001 +0100
@@ -78,43 +78,5 @@
     cd ..
 fi
 
-if [[ -e ./DECAY/decay ]]; then
-    cd DECAY
-    echo -$seed > iseed.dat
-    for ((i = 1 ;  i <= 20;  i++)) ; do
-	if [[ -e decay_$i\.in ]]; then
-	    echo "Decaying events..."
-	    mv ../events.lhe ../events_in.lhe
-	    ./decay < decay_$i\.in
-	fi
-    done
-    cd ..
-fi
-
-if [[ -e ./REPLACE/replace.pl ]]; then
-    for ((i = 1 ;  i <= 20;  i++)) ; do
-	if [[ -e ./REPLACE/replace_card$i\.dat ]];then
-	    echo "Adding flavors..."
-	    mv ./events.lhe ./events_in.lhe
-	    cd ./REPLACE
-	    ./replace.pl ../events_in.lhe ../events.lhe < replace_card$i\.dat
-	    cd ..
-	fi
-    done
-fi
-
-# part added by Stephen Mrenna to correct the kinematics of the replaced
-#  particles
-if [[ -e ./madevent/bin/internal/addmasses.py ]]; then
-  mv ./events.lhe ./events.lhe.0
-  python ./madevent/bin/internal/addmasses.py ./events.lhe.0 ./events.lhe
-  if [[ $? -eq 0 ]]; then
-     echo "Mass added"
-     rm -rf ./events.lhe.0 &> /dev/null
-  else
-     mv ./events.lhe.0 ./events.lhe
-  fi
-fi  
-
 gzip -f events.lhe
 exit
diff -ur -x '.*/models/.*' ../old/MG5_aMC_v2_3_3/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc MG5_aMC_v2_3_3/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc
--- ../old/MG5_aMC_v2_3_3/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc	2015-10-26 02:18:52.000000001 +0100
+++ MG5_aMC_v2_3_3/Template/NLO/SubProcesses/MCmasses_PYTHIA8.inc	2015-11-18 16:58:52.000000001 +0100
@@ -1,12 +1,12 @@
-      mcmass(1)=0.33d0
-      mcmass(2)=0.33d0
-      mcmass(3)=0.50d0
-      mcmass(4)=1.50d0
-      mcmass(5)=4.80d0
-      mcmass(11)=0.510998928d-3
-      mcmass(12)=0.d0
-      mcmass(13)=0.1056583715d0
-      mcmass(14)=0.d0
-      mcmass(15)=1.77682d0
-      mcmass(16)=0.d0
+      mcmass(1)=0.0d0
+      mcmass(2)=0.0d0
+      mcmass(3)=0.0d0
+      mcmass(4)=0.0d0
+      mcmass(5)=0.0d0
+      mcmass(11)=0.0d0
+      mcmass(12)=0.0d0
+      mcmass(13)=0.0d0
+      mcmass(14)=0.0d0
+      mcmass(15)=0.0d0
+      mcmass(16)=0.0d0
       mcmass(21)=0.0d0
diff -ur ../old/MG5_aMC_v2_3_3/madgraph/interface/madgraph_interface.py MG5_aMC_v2_3_2_2/madgraph/interface/madgraph_interface.py
--- ../old/MG5_aMC_v2_3_3/madgraph/interface/madgraph_interface.py       2015-09-07 20:07:39.000000000 +0200
+++ MG5_aMC_v2_3_3/madgraph/interface/madgraph_interface.py    2015-11-17 10:50:56.985751642 +0100
@@ -5851,6 +5851,9 @@
         elif args[0] in ['cluster_local_path']:
             self.options[args[0]] = args[1].strip()
 
+        elif args[0] in ['cluster_queue']:
+            self.options[args[0]] = args[1].strip()
+        
         elif args[0] == 'cluster_status_update':
             if '(' in args[1]:
                 data = ' '.join([a for a in args[1:] if not a.startswith('-')])
